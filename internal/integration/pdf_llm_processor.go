package integration

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"mime/multipart"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"github.com/freedkr/moonshot/internal/config"
	"github.com/freedkr/moonshot/internal/database"
	"github.com/freedkr/moonshot/internal/model"
	"gorm.io/datatypes"
)

// PDFLLMProcessor å¤„ç†PDFéªŒè¯å’ŒLLMè¯­ä¹‰åˆ†æçš„é›†æˆ
type PDFLLMProcessor struct {
	config        *config.Config
	httpClient    *http.Client
	db            database.DatabaseInterface
	llmServiceURL string
	pdfServiceURL string
}

// NewPDFLLMProcessor åˆ›å»ºæ–°çš„å¤„ç†å™¨
func NewPDFLLMProcessor(cfg *config.Config, db database.DatabaseInterface) *PDFLLMProcessor {
	return &PDFLLMProcessor{
		config: cfg,
		db:     db,
		httpClient: &http.Client{
			Timeout: 120 * time.Second,
		},
		llmServiceURL: getServiceURL(cfg, "llm-service", "8090"),
		pdfServiceURL: getServiceURL(cfg, "pdf-validator", "8000"),
	}
}

// ProcessWithPDFAndLLM ä½¿ç”¨æ–°çš„å¢é‡æ›´æ–°æµç¨‹å¤„ç†èŒä¸šåˆ†ç±»æ•°æ®
func (p *PDFLLMProcessor) ProcessWithPDFAndLLM(ctx context.Context, taskID string, excelPath string, categories []*model.Category) error {
	// ä½¿ç”¨æ–°çš„å¢é‡å¤„ç†å™¨æ‰§è¡Œ5æ­¥æµç¨‹
	incrementalProcessor := NewIncrementalProcessor(p.config, p.db)

	return incrementalProcessor.ProcessIncrementalFlow(ctx, taskID, excelPath, categories)
}

// ProcessWithPDFAndLLMLegacy ä¿ç•™åŸå§‹çš„åˆ é™¤é‡å»ºé€»è¾‘ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
func (p *PDFLLMProcessor) ProcessWithPDFAndLLMLegacy(ctx context.Context, taskID string, excelPath string, categories []*model.Category) error {
	// ç¬¬ä¸€æ­¥ï¼šè°ƒç”¨PDFéªŒè¯æœåŠ¡
	pdfResult, err := p.callPDFValidator(ctx, taskID, excelPath)
	if err != nil {
		return fmt.Errorf("PDFéªŒè¯å¤±è´¥: %w", err)
	}

	// ç¬¬äºŒæ­¥ï¼šç¬¬ä¸€è½®LLMè¯­ä¹‰åˆ†æ - æ¸…æ´—PDFç»“æœ
	cleanedPDFData, err := p.firstLLMAnalysis(ctx, pdfResult)
	if err != nil {
		return fmt.Errorf("ç¬¬ä¸€è½®LLMåˆ†æå¤±è´¥: %w", err)
	}

	// ç¬¬ä¸‰æ­¥ï¼šèåˆåˆå§‹è§£æç»“æœå’Œæ¸…æ´—åçš„PDFæ•°æ®
	choices := p.MergeResults(categories, cleanedPDFData)

	// ç¬¬å››æ­¥ï¼šç¬¬äºŒè½®LLMè¯­ä¹‰åˆ†æ - é€‰æ‹©æœ€ä¼˜ç»“æœ
	finalResult, err := p.SecondLLMAnalysis(ctx, choices)
	if err != nil {
		return fmt.Errorf("ç¬¬äºŒè½®LLMåˆ†æå¤±è´¥: %w", err)
	}

	// ç¬¬äº”æ­¥ï¼šä¿å­˜æœ€ç»ˆç»“æœåˆ°æ•°æ®åº“ (åˆ é™¤é‡å»ºæ–¹å¼)
	err = p.saveFinalResult(ctx, taskID, finalResult)
	if err != nil {
		return fmt.Errorf("ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: %w", err)
	}

	return nil
}

// callPDFValidator è°ƒç”¨PDFéªŒè¯æœåŠ¡
func (p *PDFLLMProcessor) callPDFValidator(ctx context.Context, taskID string, _ string) (map[string]interface{}, error) {
	// ä½¿ç”¨å›ºå®šçš„PDFæ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®
	pdfFilePath := os.Getenv("PDF_TEST_FILE_PATH")
	if pdfFilePath == "" {
		// é»˜è®¤è·¯å¾„ï¼Œå®¹å™¨å†…ä½¿ç”¨ç»å¯¹è·¯å¾„
		pdfFilePath = "/root/testdata/2025042918334715812.pdf"
		// å¦‚æœå®¹å™¨å†…è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ç›¸å¯¹è·¯å¾„ï¼ˆç”¨äºæœ¬åœ°å¼€å‘ï¼‰
		if _, err := os.Stat(pdfFilePath); os.IsNotExist(err) {
			pdfFilePath = "testdata/2025042918334715812.pdf"
		}
	}

	// è¯»å–PDFæ–‡ä»¶
	pdfFile, err := os.Open(pdfFilePath)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•æ‰“å¼€PDFæ–‡ä»¶ %s: %w", pdfFilePath, err)
	}
	defer pdfFile.Close()

	// åˆ›å»ºmultipartè¯·æ±‚
	body := &bytes.Buffer{}
	writer := multipart.NewWriter(body)

	// æ·»åŠ æ–‡ä»¶å­—æ®µ
	part, err := writer.CreateFormFile("file", filepath.Base(pdfFilePath))
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºformæ–‡ä»¶å¤±è´¥: %w", err)
	}

	if _, err := io.Copy(part, pdfFile); err != nil {
		return nil, fmt.Errorf("å¤åˆ¶æ–‡ä»¶å†…å®¹å¤±è´¥: %w", err)
	}

	// æ·»åŠ validation_typeå­—æ®µ
	if err := writer.WriteField("validation_type", "standard"); err != nil {
		return nil, fmt.Errorf("å†™å…¥validation_typeå¤±è´¥: %w", err)
	}

	// å…³é—­writer
	if err := writer.Close(); err != nil {
		return nil, fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// è°ƒç”¨upload-and-validateæ¥å£
	url := fmt.Sprintf("http://%s/api/v1/upload-and-validate", p.pdfServiceURL)
	req, err := http.NewRequestWithContext(ctx, "POST", url, body)
	if err != nil {
		return nil, err
	}
	req.Header.Set("Content-Type", writer.FormDataContentType())

	// å‘é€è¯·æ±‚
	resp, err := p.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("è°ƒç”¨PDFéªŒè¯æœåŠ¡å¤±è´¥: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusAccepted {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("PDFæœåŠ¡è¿”å›é”™è¯¯ %d: %s", resp.StatusCode, string(body))
	}

	// è·å–éªŒè¯ä»»åŠ¡ID
	var validationResp map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&validationResp); err != nil {
		return nil, err
	}

	pdfTaskID := validationResp["task_id"].(string)

	// ç­‰å¾…å¤„ç†å®Œæˆ
	if err := p.waitForPDFCompletion(ctx, pdfTaskID); err != nil {
		return nil, err
	}

	// è·å–èŒä¸šç¼–ç ç»“æœ
	return p.getOccupationCodes(ctx, pdfTaskID)
}

// waitForPDFCompletion ç­‰å¾…PDFå¤„ç†å®Œæˆ
func (p *PDFLLMProcessor) waitForPDFCompletion(ctx context.Context, pdfTaskID string) error {
	ticker := time.NewTicker(3 * time.Second) // å¢åŠ è½®è¯¢é—´éš”åˆ°3ç§’
	defer ticker.Stop()

	timeout := time.After(180 * time.Second) // å¢åŠ åˆ°3åˆ†é’Ÿè¶…æ—¶ï¼Œç»™PDFå¤„ç†æ›´å¤šæ—¶é—´

	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-timeout:
			// è¶…æ—¶åï¼Œå°è¯•ç›´æ¥è·å–ç»“æœï¼Œå¯èƒ½å·²ç»å®Œæˆä½†statusæ¥å£æœ‰é—®é¢˜
			return nil // è¿”å›nilè®©è°ƒç”¨æ–¹å°è¯•è·å–ç»“æœ
		case <-ticker.C:
			// å°è¯•æ£€æŸ¥çŠ¶æ€ï¼Œå¦‚æœå¤±è´¥åˆ™ç»§ç»­ç­‰å¾…
			if completed, err := p.checkPDFStatus(ctx, pdfTaskID); err != nil {
				// å¿½ç•¥statusæ¥å£çš„é”™è¯¯ï¼Œç»§ç»­ç­‰å¾…
				continue
			} else if completed {
				return nil
			}
		}
	}
}

// checkPDFStatus æ£€æŸ¥PDFå¤„ç†çŠ¶æ€
func (p *PDFLLMProcessor) checkPDFStatus(ctx context.Context, pdfTaskID string) (bool, error) {
	url := fmt.Sprintf("http://%s/api/v1/status/%s", p.pdfServiceURL, pdfTaskID)

	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return false, err
	}

	resp, err := p.httpClient.Do(req)
	if err != nil {
		return false, err
	}
	defer resp.Body.Close()

	// å¦‚æœçŠ¶æ€æ¥å£è¿”å›500ï¼ˆå¯èƒ½æ˜¯DateTimeåºåˆ—åŒ–é—®é¢˜ï¼‰ï¼Œå‡è®¾è¿˜åœ¨å¤„ç†ä¸­
	if resp.StatusCode == http.StatusInternalServerError {
		return false, nil // è¿”å›æœªå®Œæˆï¼Œç»§ç»­ç­‰å¾…
	}

	if resp.StatusCode != http.StatusOK {
		return false, fmt.Errorf("çŠ¶æ€ç : %d", resp.StatusCode)
	}

	var status map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&status); err != nil {
		return false, err
	}

	statusStr, _ := status["status"].(string)
	switch statusStr {
	case "completed":
		return true, nil
	case "failed", "error":
		errorMsg, _ := status["error"].(string)
		return false, fmt.Errorf("PDFéªŒè¯å¤±è´¥: %s", errorMsg)
	default:
		return false, nil // è¿˜åœ¨å¤„ç†ä¸­
	}
}

// getOccupationCodes è·å–èŒä¸šç¼–ç ç»“æœ
func (p *PDFLLMProcessor) getOccupationCodes(ctx context.Context, pdfTaskID string) (map[string]interface{}, error) {
	// è°ƒç”¨occupation-codesæ¥å£è·å–ç»“æœ
	url := fmt.Sprintf("http://%s/api/v1/blocks/%s/occupation-codes", p.pdfServiceURL, pdfTaskID)

	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return nil, err
	}

	resp, err := p.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("è·å–èŒä¸šç¼–ç ç»“æœå¤±è´¥: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("è·å–ç»“æœå¤±è´¥ %d: %s", resp.StatusCode, string(body))
	}

	var result map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, fmt.Errorf("è§£æç»“æœå¤±è´¥: %w", err)
	}

	return result, nil
}

// firstLLMAnalysis ç¬¬ä¸€è½®LLMåˆ†æ - æ¸…æ´—PDFè§£æç»“æœï¼ˆä½¿ç”¨å¹¶å‘ï¼‰
func (p *PDFLLMProcessor) firstLLMAnalysis(ctx context.Context, pdfData map[string]interface{}) ([]map[string]interface{}, error) {
	fmt.Printf("ğŸš€ [FirstLLMAnalysis-å¼€å§‹] pdfData keysæ•°é‡: %d\n", len(pdfData))
	
	// æ‰“å°PDFæ•°æ®çš„ç»“æ„
	for key, value := range pdfData {
		if key == "occupation_codes" {
			if codes, ok := value.([]interface{}); ok {
				fmt.Printf("  ğŸ“Š [PDFæ•°æ®] occupation_codes æ•°é‡: %d\n", len(codes))
				if len(codes) > 0 && len(codes) <= 3 {
					// æ‰“å°å‰å‡ ä¸ªç¤ºä¾‹
					for i, code := range codes {
						if i >= 3 {
							break
						}
						fmt.Printf("    ç¤ºä¾‹%d: %+v\n", i+1, code)
					}
				}
			}
		} else {
			fmt.Printf("  ğŸ”‘ [PDFæ•°æ®] %s: %v\n", key, value)
		}
	}
	
	// ä½¿ç”¨æ‰¹é‡å¤„ç†å™¨è¿›è¡Œå¹¶å‘å¤„ç†
	batchProcessor := NewBatchProcessor(p)
	fmt.Printf("ğŸ”„ [FirstLLMAnalysis] åˆ›å»ºBatchProcessorï¼Œå‡†å¤‡å¹¶å‘å¤„ç†\n")

	// å¹¶å‘å¤„ç†PDFæ•°æ®ï¼ŒæŒ‰ç¼–ç å‰ç¼€ï¼ˆ1-xx, 2-xxç­‰ï¼‰åˆ†ç»„
	fmt.Printf("ğŸ¤– [FirstLLMAnalysis] å¼€å§‹è°ƒç”¨LLMè¿›è¡Œæ•°æ®æ¸…æ´—...\n")
	cleanedData, err := batchProcessor.ProcessPDFDataConcurrently(ctx, pdfData)
	
	if err != nil {
		fmt.Printf("âŒ [FirstLLMAnalysis-å¹¶å‘å¤±è´¥] é”™è¯¯: %v, å›é€€åˆ°å•æ¬¡å¤„ç†\n", err)
		// å¦‚æœå¹¶å‘å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å•æ¬¡å¤„ç†
		return p.firstLLMAnalysisFallback(ctx, pdfData)
	}

	fmt.Printf("âœ… [FirstLLMAnalysis-æˆåŠŸ] æ¸…æ´—åæ•°æ®æ¡æ•°: %d\n", len(cleanedData))
	
	// æ‰“å°å‰3æ¡æ¸…æ´—åçš„æ•°æ®ç¤ºä¾‹
	for i, data := range cleanedData {
		if i >= 3 {
			break
		}
		fmt.Printf("  ğŸ“ [æ¸…æ´—ç»“æœ%d] %+v\n", i+1, data)
	}
	
	return cleanedData, nil
}

// firstLLMAnalysisFallback ç¬¬ä¸€è½®LLMåˆ†æçš„å›é€€æ–¹æ¡ˆï¼ˆå•æ¬¡å¤„ç†ï¼‰
func (p *PDFLLMProcessor) firstLLMAnalysisFallback(ctx context.Context, pdfData map[string]interface{}) ([]map[string]interface{}, error) {
	// å…ˆæå–æ ¸å¿ƒå­—æ®µ(åªåŒ…å«codeå’Œname)ï¼Œé¿å…tokené™åˆ¶
	coreData := extractCoreFields(pdfData)

	// è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•æ ¸å¿ƒå­—æ®µæå–æƒ…å†µ
	fmt.Printf("DEBUG: firstLLMAnalysisFallback æå–äº†æ ¸å¿ƒå­—æ®µï¼ˆåªåŒ…å«codeå’Œnameï¼‰\n")

	prompt := fmt.Sprintf(`ä½ æ˜¯ä¸€åæ•°æ®æ¸…æ´—ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ä»PDFæå–çš„èŒä¸šåˆ†ç±»æ•°æ®ï¼Œè¯†åˆ«å¹¶æå–å‡†ç¡®çš„èŒä¸šç¼–ç å’Œåç§°ã€‚

PDFæå–çš„æ ¸å¿ƒæ•°æ®ï¼ˆå·²è¿‡æ»¤åªåŒ…å«codeå’Œnameï¼‰ï¼š
%s

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™è¿›è¡Œæ¸…æ´—ï¼š
1. è¯†åˆ«æ‰€æœ‰æœ‰æ•ˆçš„èŒä¸šç¼–ç ï¼ˆæ ¼å¼å¦‚ï¼š1-01-01-01ï¼‰
2. ä¸ºæ¯ä¸ªç¼–ç åŒ¹é…æœ€å‡†ç¡®çš„èŒä¸šåç§°
3. å»é™¤æè¿°æ€§æ–‡å­—å’Œæ— å…³å†…å®¹
4. ä¿®æ­£æ˜æ˜¾çš„OCRè¯†åˆ«é”™è¯¯

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
{
  "code": "èŒä¸šç¼–ç ",
  "name": "èŒä¸šåç§°",
  "confidence": "ç½®ä¿¡åº¦(0-1)",
  "source": "pdf"
}

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚`, jsonString(coreData))

	result, err := p.callLLMService(ctx, "data_cleaning", prompt)
	if err != nil {
		return nil, err
	}

	// æ‰“å°åŸå§‹LLMè¿”å›ç»“æœä»¥ä¾¿è°ƒè¯•
	fmt.Printf("ğŸ” [LLMåŸå§‹å“åº”] é•¿åº¦=%d\n", len(result))
	if len(result) > 0 {
		// æ‰“å°å‰500ä¸ªå­—ç¬¦å’Œå500ä¸ªå­—ç¬¦
		if len(result) <= 1000 {
			fmt.Printf("ğŸ“ [LLMå®Œæ•´å“åº”]:\n%s\n", result)
		} else {
			fmt.Printf("ğŸ“ [LLMå“åº”å¼€å¤´500å­—ç¬¦]:\n%s\n", result[:500])
			fmt.Printf("ğŸ“ [LLMå“åº”ç»“å°¾500å­—ç¬¦]:\n%s\n", result[len(result)-500:])
		}
	}

	// è§£æç»“æœ - å¤„ç†ä¸‰ç§æ ¼å¼ï¼š{"items": [...]}ã€ç›´æ¥JSONæ•°ç»„ã€æˆ–JSONå­—ç¬¦ä¸²
	var cleanedData []map[string]interface{}
	
	// é¦–å…ˆå°è¯•è§£æä¸º {"items": [...]} æ ¼å¼
	var resultWrapper struct {
		Items []map[string]interface{} `json:"items"`
	}
	// å…ˆå°è¯•æ¸…ç†å¯èƒ½çš„éJSONå†…å®¹
	cleanResult := extractJSON(result)
	fmt.Printf("ğŸ” [æ¸…ç†åJSON] é•¿åº¦=%d\n", len(cleanResult))
	if cleanResult != result {
		fmt.Printf("ğŸ“ [æ¸…ç†åJSONå†…å®¹]:\n%s\n", cleanResult)
	}
	
	if err := json.Unmarshal([]byte(cleanResult), &resultWrapper); err != nil {
		fmt.Printf("âš ï¸ [wrapperæ ¼å¼è§£æå¤±è´¥] é”™è¯¯: %v\n", err)
		// å¦‚æœåŒ…è£…æ ¼å¼å¤±è´¥ï¼Œå°è¯•è§£æä¸ºç›´æ¥çš„JSONæ•°ç»„
		if err2 := json.Unmarshal([]byte(cleanResult), &cleanedData); err2 != nil {
			fmt.Printf("âš ï¸ [æ•°ç»„æ ¼å¼è§£æå¤±è´¥] é”™è¯¯: %v\n", err2)
			// å¦‚æœç›´æ¥æ•°ç»„ä¹Ÿå¤±è´¥ï¼Œå°è¯•è§£æä¸ºJSONå­—ç¬¦ä¸²ï¼ˆåŒé‡ç¼–ç çš„æƒ…å†µï¼‰
			var jsonString string
			if err3 := json.Unmarshal([]byte(cleanResult), &jsonString); err3 != nil {
				fmt.Printf("âŒ [å­—ç¬¦ä¸²æ ¼å¼è§£æå¤±è´¥] é”™è¯¯: %v\n", err3)
				fmt.Printf("âŒ [JSONè§£æå…¨éƒ¨å¤±è´¥] ä¸‰ç§æ ¼å¼éƒ½æ— æ³•è§£æ\n")
				
				// å°è¯•ä½¿ç”¨æ›´å®½æ¾çš„è§£ææ–¹å¼
				fmt.Printf("ğŸ”„ [å°è¯•å®½æ¾è§£æ] å°è¯•è§£æéƒ¨åˆ†æ•°æ®...\n")
				if partialData := tryParsePartialJSON(cleanResult); partialData != nil && len(partialData) > 0 {
					fmt.Printf("âœ… [éƒ¨åˆ†è§£ææˆåŠŸ] è§£æå‡º %d æ¡æ•°æ®\n", len(partialData))
					return partialData, nil
				}
				
				return nil, fmt.Errorf("è§£æLLMè¿”å›ç»“æœå¤±è´¥: wrapper_err=%v, array_err=%v, string_err=%v", err, err2, err3)
			}
			fmt.Printf("ğŸ”„ [åŒé‡ç¼–ç ] æ£€æµ‹åˆ°JSONå­—ç¬¦ä¸²ï¼Œå°è¯•äºŒæ¬¡è§£æ...\n")
			// è§£æJSONå­—ç¬¦ä¸²ä¸­çš„å®é™…JSON
			if err4 := json.Unmarshal([]byte(jsonString), &resultWrapper); err4 != nil {
				if err5 := json.Unmarshal([]byte(jsonString), &cleanedData); err5 != nil {
					fmt.Printf("âŒ [äºŒæ¬¡è§£æå¤±è´¥] é”™è¯¯: %v\n", err5)
					return nil, fmt.Errorf("è§£æJSONå­—ç¬¦ä¸²å¤±è´¥: %v", err5)
				}
				fmt.Printf("âœ… [äºŒæ¬¡è§£ææˆåŠŸ] è§£æä¸ºJSONæ•°ç»„\n")
			} else {
				cleanedData = resultWrapper.Items
				fmt.Printf("âœ… [äºŒæ¬¡è§£ææˆåŠŸ] è§£æä¸ºwrapperæ ¼å¼\n")
			}
		} else {
			fmt.Printf("âœ… [è§£ææˆåŠŸ] ç›´æ¥è§£æä¸ºJSONæ•°ç»„\n")
		}
	} else {
		cleanedData = resultWrapper.Items
		fmt.Printf("âœ… [è§£ææˆåŠŸ] è§£æä¸ºwrapperæ ¼å¼\n")
	}
	
	fmt.Printf("ğŸ“Š [è§£æç»“æœ] æˆåŠŸè§£æ %d æ¡æ•°æ®\n", len(cleanedData))

	return cleanedData, nil
}

// tryParsePartialJSON å°è¯•è§£æéƒ¨åˆ†JSONæ•°æ®ï¼ˆå®½æ¾æ¨¡å¼ï¼‰
func tryParsePartialJSON(input string) []map[string]interface{} {
	// å°è¯•æ‰¾åˆ°JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
	input = strings.TrimSpace(input)
	
	// å¦‚æœæ˜¯ä¸å®Œæ•´çš„JSONï¼Œå°è¯•ä¿®å¤
	if strings.HasPrefix(input, "[") && !strings.HasSuffix(input, "]") {
		// å°è¯•è¡¥å…¨ç»“å°¾
		input = input + "]"
	}
	
	// å°è¯•è§£æ
	var result []map[string]interface{}
	decoder := json.NewDecoder(strings.NewReader(input))
	decoder.UseNumber() // ä½¿ç”¨Numberç±»å‹é¿å…ç²¾åº¦é—®é¢˜
	
	if err := decoder.Decode(&result); err != nil {
		// å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•é€è¡Œè§£æ
		fmt.Printf("âš ï¸ [å®½æ¾è§£æ] å®Œæ•´è§£æå¤±è´¥ï¼Œå°è¯•é€è¡Œè§£æ...\n")
		
		lines := strings.Split(input, "\n")
		for _, line := range lines {
			line = strings.TrimSpace(line)
			if line == "" || line == "[" || line == "]" || line == "," {
				continue
			}
			
			// ç§»é™¤å¯èƒ½çš„é€—å·ç»“å°¾
			line = strings.TrimSuffix(line, ",")
			
			var item map[string]interface{}
			if err := json.Unmarshal([]byte(line), &item); err == nil {
				result = append(result, item)
			}
		}
	}
	
	return result
}

// SecondLLMAnalysis ç¬¬äºŒè½®LLMåˆ†æ - ä½¿ç”¨ä»»åŠ¡ç±»å‹è½®è¯¢å®ç°å¹¶å‘ï¼ˆå¯¼å‡ºä¾›æµ‹è¯•ï¼‰
func (p *PDFLLMProcessor) SecondLLMAnalysis(ctx context.Context, choices []SemanticChoiceItem) ([]map[string]interface{}, error) {
	fmt.Printf("ğŸ¤– [SecondLLMAnalysis-å¼€å§‹] å¼€å§‹ç¬¬äºŒè½®LLMåˆ†æï¼Œå¾…å¤„ç†æ¡ç›®æ•°: %d\n", len(choices))
	// å®šä¹‰å¯ç”¨çš„ä»»åŠ¡ç±»å‹æ± ï¼Œåªä½¿ç”¨LLMæœåŠ¡å·²é…ç½®è·¯ç”±çš„ç±»å‹
	taskTypes := []string{
		"semantic_analysis", // ä¸»è¦ç”¨äºè¯­ä¹‰åˆ†æ
		"data_cleaning",     // å¤ç”¨æ•°æ®æ¸…æ´—é˜Ÿåˆ—
	}

	// ç»“æœæ”¶é›†
	type itemResult struct {
		index  int
		result map[string]interface{}
		err    error
	}

	resultCh := make(chan itemResult, len(choices))

	// ä½¿ç”¨goroutineæ± å¤„ç†ï¼Œæ¯ä¸ªgoroutineä½¿ç”¨ä¸åŒçš„ä»»åŠ¡ç±»å‹
	var wg sync.WaitGroup
	for i, choice := range choices {
		wg.Add(1)
		// è½®è¯¢åˆ†é…ä»»åŠ¡ç±»å‹
		taskType := taskTypes[i%len(taskTypes)]

		go func(idx int, item SemanticChoiceItem, tType string) {
			defer wg.Done()

			// å•æ¡å¤„ç†ï¼Œä½¿ç”¨åˆ†é…çš„ä»»åŠ¡ç±»å‹
			result, err := p.analyzeSingleChoice(ctx, item, tType)
			resultCh <- itemResult{
				index:  idx,
				result: result,
				err:    err,
			}
		}(i, choice, taskType)
	}

	// ç­‰å¾…æ‰€æœ‰goroutineå®Œæˆ
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// æ”¶é›†ç»“æœå¹¶ä¿æŒé¡ºåº
	results := make([]map[string]interface{}, len(choices))
	errorCount := 0

	for res := range resultCh {
		if res.err != nil {
			errorCount++
			fmt.Printf("  âŒ [LLMå¤„ç†å¤±è´¥] æ¡ç›® %d (Code=%s) å¤±è´¥: %v\n", 
				res.index, choices[res.index].Code, res.err)
			// ä½¿ç”¨é»˜è®¤å€¼
			results[res.index] = map[string]interface{}{
				"code":        choices[res.index].Code,
				"name":        choices[res.index].RuleName, // é»˜è®¤ä½¿ç”¨è§„åˆ™åç§°
				"level":       "ç»†ç±»",
				"parent_code": inferParentCode(choices[res.index].Code),
			}
		} else {
			if res.index < 3 { // æ‰“å°å‰3ä¸ªæˆåŠŸçš„ç»“æœ
				fmt.Printf("  âœ… [LLMå¤„ç†æˆåŠŸ] æ¡ç›® %d (Code=%s): %+v\n", 
					res.index, choices[res.index].Code, res.result)
			}
			results[res.index] = res.result
		}
	}

	fmt.Printf("ğŸ“Š [SecondLLMAnalysis-ç»Ÿè®¡] æ€»æ¡ç›®=%d, æˆåŠŸ=%d, å¤±è´¥=%d\n", 
		len(choices), len(choices)-errorCount, errorCount)
	
	if errorCount > len(choices)/2 {
		fmt.Printf("âŒ [SecondLLMAnalysis-é”™è¯¯] è¶…è¿‡50%%çš„æ¡ç›®å¤„ç†å¤±è´¥\n")
		return results, fmt.Errorf("è¶…è¿‡50%%çš„æ¡ç›®å¤„ç†å¤±è´¥(%d/%d)", errorCount, len(choices))
	}

	// è¿‡æ»¤æ‰nilç»“æœ
	var finalResults []map[string]interface{}
	for _, r := range results {
		if r != nil {
			finalResults = append(finalResults, r)
		}
	}

	fmt.Printf("âœ… [SecondLLMAnalysis-å®Œæˆ] è¿”å›æœ‰æ•ˆç»“æœ=%dæ¡\n", len(finalResults))
	return finalResults, nil
}

// analyzeSingleChoice åˆ†æå•ä¸ªé€‰æ‹©é¡¹ï¼Œä½¿ç”¨æŒ‡å®šçš„ä»»åŠ¡ç±»å‹
func (p *PDFLLMProcessor) analyzeSingleChoice(ctx context.Context, choice SemanticChoiceItem, taskType string) (map[string]interface{}, error) {
	// æ„å»ºå•æ¡æ•°æ®çš„ç²¾ç¡®æç¤º
	prompt := fmt.Sprintf(`ä½ æ˜¯èŒä¸šåˆ†ç±»ä¸“å®¶.è¯·ä¸ºä»¥ä¸‹èŒä¸šç¼–ç é€‰æ‹©æœ€åˆé€‚çš„åç§°:

ç¼–ç :%s
é€‰é¡¹1:%s
é€‰é¡¹2:%s
çˆ¶çº§ç±»åˆ«:%s

é€‰æ‹©è§„åˆ™:
- åªèƒ½é€‰æ‹©é€‰é¡¹1æˆ–é€‰é¡¹2,ä¸èƒ½åˆ›é€ æ–°åç§°ã€‚
- é€‰æ‹©ä¸çˆ¶çº§å±‚æ¬¡è¯­ä¹‰æ›´è¿è´¯çš„åç§°
- ä¼˜å…ˆé€‰æ‹©å®Œæ•´çš„ã€åè¯æ€§çš„èŒä¸šåç§°
- å¦‚æœä¸¤ä¸ªåç§°ç›¸ä¼¼,é€‰æ‹©æ›´å®Œæ•´ã€æ›´è§„èŒƒçš„ç‰ˆæœ¬
- æ’é™¤åŒ…å«"æœ¬å°ç±»åŒ…æ‹¬"ã€"è¿›è¡Œ..."ã€"æ‹…ä»»..."ç­‰æè¿°æ€§çŸ­è¯­

è¿”å›JSONæ ¼å¼:
{
  "code": "ç¼–ç ",
  "name": "é€‰æ‹©åçš„åç§°",
  "parent_name": "çˆ¶çº§ç±»åˆ«åç§°"
}`,
		choice.Code,
		choice.RuleName,
		choice.PdfName,
		choice.ParentHierarchy)

	// ä½¿ç”¨æŒ‡å®šçš„ä»»åŠ¡ç±»å‹è°ƒç”¨LLMæœåŠ¡
	result, err := p.callLLMServiceWithRetry(ctx, taskType, prompt, 3)
	if err != nil {
		return nil, err
	}

	// è§£æç»“æœ
	var singleResult map[string]interface{}
	if err := json.Unmarshal([]byte(result), &singleResult); err != nil {
		// å°è¯•æå–JSON
		result = extractJSON(result)
		if err := json.Unmarshal([]byte(result), &singleResult); err != nil {
			return nil, fmt.Errorf("è§£æç»“æœå¤±è´¥: %w", err)
		}
	}

	// éªŒè¯å’Œè¡¥å……å¿…è¦å­—æ®µ
	if code, ok := singleResult["code"].(string); !ok || code != choice.Code {
		singleResult["code"] = choice.Code
	}

	if _, ok := singleResult["name"].(string); !ok {
		// å¦‚æœLLMæ²¡æœ‰æ­£ç¡®è¿”å›ï¼Œä½¿ç”¨è§„åˆ™åç§°ä½œä¸ºé»˜è®¤å€¼
		singleResult["name"] = choice.RuleName
	}

	if _, ok := singleResult["level"].(string); !ok {
		singleResult["level"] = "ç»†ç±»"
	}

	if _, ok := singleResult["parent_code"].(string); !ok {
		singleResult["parent_code"] = inferParentCode(choice.Code)
	}

	return singleResult, nil
}

// inferParentCode ä»ç¼–ç æ¨æ–­çˆ¶ç¼–ç 
func inferParentCode(code string) string {
	// ä¾‹å¦‚ï¼š"1-01-01-01" -> "1-01-01"
	parts := strings.Split(code, "-")
	if len(parts) > 1 {
		return strings.Join(parts[:len(parts)-1], "-")
	}
	return ""
}

// extractJSON ä»LLMå“åº”ä¸­æå–JSONéƒ¨åˆ†
func extractJSON(response string) string {
	// æŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
	start := strings.Index(response, "[")
	end := strings.LastIndex(response, "]")

	if start != -1 && end != -1 && end > start {
		return response[start : end+1]
	}

	// æŸ¥æ‰¾JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
	start = strings.Index(response, "{")
	end = strings.LastIndex(response, "}")

	if start != -1 && end != -1 && end > start {
		return response[start : end+1]
	}

	return response
}

// SemanticChoiceItem è¯­ä¹‰é€‰æ‹©é¡¹ç»“æ„
type SemanticChoiceItem struct {
	Code            string `json:"code"`
	RuleName        string `json:"rule_name"`
	PdfName         string `json:"pdf_name"`
	ParentHierarchy string `json:"parent_hierarchy"`
}

// MergeResults èåˆè§„åˆ™è§£æç»“æœå’ŒPDFæ¸…æ´—ç»“æœä¸ºè¯­ä¹‰é€‰æ‹©ç»“æ„ï¼ˆå¯¼å‡ºä¾›æµ‹è¯•ï¼‰
func (p *PDFLLMProcessor) MergeResults(categories []*model.Category, pdfData []map[string]interface{}) []SemanticChoiceItem {
	// æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„ - åªè®°å½•ç›´æ¥çˆ¶çº§åç§°
	parentNameMap := make(map[string]string)
	var buildParentMap func([]*model.Category, string)
	buildParentMap = func(cats []*model.Category, parentName string) {
		for _, cat := range cats {
			if cat == nil {
				continue
			}

			// è®°å½•å½“å‰èŠ‚ç‚¹çš„çˆ¶çº§åç§°
			if parentName != "" {
				parentNameMap[cat.Code] = parentName
			}

			// é€’å½’å¤„ç†å­èŠ‚ç‚¹ï¼Œå½“å‰èŠ‚ç‚¹åç§°ä½œä¸ºå­èŠ‚ç‚¹çš„çˆ¶çº§åç§°
			if len(cat.Children) > 0 {
				buildParentMap(cat.Children, cat.Name)
			}
		}
	}
	buildParentMap(categories, "")

	// æ”¶é›†éª¨æ¶æ•°æ® - åªæ”¶é›†ç»†ç±»ï¼ˆæœ€é•¿ç¼–ç ï¼‰
	ruleData := make(map[string]string)
	var collectDetailedCodes func([]*model.Category)
	collectDetailedCodes = func(cats []*model.Category) {
		for _, cat := range cats {
			if cat == nil {
				continue
			}

			// å¦‚æœæ˜¯å¶å­èŠ‚ç‚¹ï¼ˆç»†ç±»ï¼‰ï¼Œæ”¶é›†
			if len(cat.Children) == 0 {
				ruleData[cat.Code] = cat.Name
			} else {
				collectDetailedCodes(cat.Children)
			}
		}
	}
	collectDetailedCodes(categories)

	// æ”¶é›†PDFæ•°æ®
	pdfDataMap := make(map[string]string)
	for _, pdfItem := range pdfData {
		code := pdfItem["code"].(string)
		name := pdfItem["name"].(string)
		pdfDataMap[code] = name
	}

	// åˆ›å»ºè¯­ä¹‰é€‰æ‹©é¡¹
	var choices []SemanticChoiceItem

	// åˆå¹¶æ‰€æœ‰æœ‰æ•°æ®çš„ç¼–ç 
	allCodes := make(map[string]bool)
	for code := range ruleData {
		allCodes[code] = true
	}
	for code := range pdfDataMap {
		allCodes[code] = true
	}

	for code := range allCodes {
		// è·å–ç›´æ¥çˆ¶çº§åç§°
		parentName := parentNameMap[code] // å¦‚æœæ²¡æœ‰çˆ¶çº§åˆ™ä¸ºç©ºå­—ç¬¦ä¸²

		choice := SemanticChoiceItem{
			Code:            code,
			RuleName:        ruleData[code],   // å¦‚æœæ²¡æœ‰åˆ™ä¸ºç©º
			PdfName:         pdfDataMap[code], // å¦‚æœæ²¡æœ‰åˆ™ä¸ºç©º
			ParentHierarchy: parentName,       // åªåŒ…å«ç›´æ¥çˆ¶çº§åç§°
		}

		// åªæœ‰è‡³å°‘æœ‰ä¸€ä¸ªåç§°æ‰åŠ å…¥
		if choice.RuleName != "" || choice.PdfName != "" {
			choices = append(choices, choice)
		}
	}

	return choices
}

// callLLMService è°ƒç”¨LLMæœåŠ¡ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼‰
func (p *PDFLLMProcessor) callLLMService(ctx context.Context, taskType string, prompt string) (string, error) {
	// ä½¿ç”¨å¸¦é‡è¯•çš„å¼‚æ­¥è°ƒç”¨
	return p.callLLMServiceWithRetry(ctx, taskType, prompt, 3)
}

// saveFinalResult ä¿å­˜æœ€ç»ˆç»“æœåˆ°æ•°æ®åº“
func (p *PDFLLMProcessor) saveFinalResult(ctx context.Context, taskID string, finalData []map[string]interface{}) error {
	// å…ˆåˆ é™¤æ—§çš„åˆ†ç±»æ•°æ® - é€šè¿‡ç›´æ¥ä½¿ç”¨GORM
	pgDB, ok := p.db.(*database.PostgreSQLDB)
	if !ok {
		return fmt.Errorf("æ•°æ®åº“ç±»å‹é”™è¯¯")
	}

	// åˆ é™¤æ—§æ•°æ®
	if err := pgDB.GetDB().WithContext(ctx).Where("task_id = ?", taskID).Delete(&database.Category{}).Error; err != nil {
		return fmt.Errorf("åˆ é™¤æ—§åˆ†ç±»æ•°æ®å¤±è´¥: %w", err)
	}

	// è½¬æ¢å¹¶ä¿å­˜æ–°çš„åˆ†ç±»æ•°æ®
	var categories []*database.Category
	for _, item := range finalData {
		// Level å­—æ®µåº”è¯¥æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆå¤§ç±»/ä¸­ç±»/å°ç±»/ç»†ç±»ï¼‰
		levelStr, ok := item["level"].(string)
		if !ok {
			// å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æ ¹æ®codeæ¨æ–­
			code := item["code"].(string)
			levelStr = inferLevelFromCode(code)
		}

		cat := &database.Category{
			TaskID:     taskID,
			Code:       item["code"].(string),
			Name:       item["name"].(string),
			Level:      levelStr,
			ParentCode: "",
		}

		if parentCode, ok := item["parent_code"].(string); ok {
			cat.ParentCode = parentCode
		}

		categories = append(categories, cat)
	}

	// æ‰¹é‡æ’å…¥ - ä½¿ç”¨ä¸Šé¢å·²ç»è·å–çš„pgDB
	if err := pgDB.GetDB().WithContext(ctx).CreateInBatches(categories, 100).Error; err != nil {
		return fmt.Errorf("æ‰¹é‡æ’å…¥å¤±è´¥: %w", err)
	}

	// æ›´æ–°ä»»åŠ¡çŠ¶æ€
	task, err := p.db.GetTask(ctx, taskID)
	if err != nil {
		return err
	}

	task.Status = "llm_processed"
	resultJSON, _ := json.Marshal(map[string]interface{}{
		"status":           "completed",
		"message":          "LLMè¯­ä¹‰åˆ†æå®Œæˆ",
		"total_categories": len(categories),
	})
	task.Result = datatypes.JSON(resultJSON)
	task.UpdatedAt = time.Now()

	return p.db.UpdateTask(ctx, task)
}

// getServiceURL è·å–æœåŠ¡URL
func getServiceURL(cfg *config.Config, serviceName string, defaultPort string) string {
	// æ ¹æ®æœåŠ¡åç§°è¿”å›å¯¹åº”çš„URL
	switch serviceName {
	case "llm-service":
		// ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œç„¶åæ˜¯é…ç½®æ–‡ä»¶
		if llmURL := os.Getenv("LLM_SERVICE_URL"); llmURL != "" {
			return llmURL
		}
		if cfg.LLM.ServiceURL != "" {
			return cfg.LLM.ServiceURL
		}
		return fmt.Sprintf("llm-service:%s", defaultPort)
	case "pdf-validator":
		// PDFéªŒè¯æœåŠ¡åœ°å€ï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®
		if pdfURL := os.Getenv("PDF_VALIDATOR_URL"); pdfURL != "" {
			return pdfURL
		}
		return fmt.Sprintf("pdf-validator:%s", defaultPort)
	default:
		return fmt.Sprintf("localhost:%s", defaultPort)
	}
}

// inferLevelFromCode æ ¹æ®ç¼–ç æ¨æ–­å±‚çº§
func inferLevelFromCode(code string) string {
	// æ ¹æ®ç¼–ç æ ¼å¼åˆ¤æ–­å±‚çº§
	// ç§»é™¤æ‰€æœ‰éæ•°å­—å’Œè¿å­—ç¬¦çš„å­—ç¬¦
	parts := strings.Split(code, "-")

	switch len(parts) {
	case 1:
		// 1ä½æ•°å­—ç¼–ç ä¸ºå¤§ç±»
		return "å¤§ç±»"
	case 2:
		// å¦‚ "1-01" ä¸ºä¸­ç±»
		return "ä¸­ç±»"
	case 3:
		// å¦‚ "1-01-01" ä¸ºå°ç±»
		return "å°ç±»"
	case 4:
		// å¦‚ "1-01-01-01" ä¸ºç»†ç±»
		return "ç»†ç±»"
	default:
		// é»˜è®¤ä¸ºç»†ç±»
		return "ç»†ç±»"
	}
}

// truncatePDFData æˆªæ–­PDFæ•°æ®ä»¥é¿å…tokené™åˆ¶
func (p *PDFLLMProcessor) truncatePDFData(pdfData map[string]interface{}, maxChars int) map[string]interface{} {
	truncated := make(map[string]interface{})

	// å¤åˆ¶åŸºæœ¬å­—æ®µ
	if taskID, ok := pdfData["task_id"]; ok {
		truncated["task_id"] = taskID
	}
	if totalFound, ok := pdfData["total_found"]; ok {
		truncated["total_found"] = totalFound
	}

	// å¤„ç†èŒä¸šç¼–ç æ•°ç»„ï¼Œåªå–å‰é¢éƒ¨åˆ†
	if codes, ok := pdfData["occupation_codes"].([]interface{}); ok {
		var truncatedCodes []interface{}
		currentSize := 0

		for _, code := range codes {
			codeJSON, _ := json.Marshal(code)
			if currentSize+len(codeJSON) > maxChars {
				break
			}
			truncatedCodes = append(truncatedCodes, code)
			currentSize += len(codeJSON)
		}

		truncated["occupation_codes"] = truncatedCodes
		truncated["_truncated"] = len(truncatedCodes) < len(codes)
		truncated["_original_count"] = len(codes)
		truncated["_processed_count"] = len(truncatedCodes)
	}

	return truncated
}

// jsonString å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
func jsonString(v interface{}) string {
	b, _ := json.MarshalIndent(v, "", "  ")
	return string(b)
}
