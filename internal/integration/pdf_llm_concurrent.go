package integration

import (
	"context"
	"encoding/json"
	"fmt"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/freedkr/moonshot/internal/model"
)

// BatchProcessor PDFå’ŒLLMæ‰¹é‡å¹¶å‘å¤„ç†å™¨
type BatchProcessor struct {
	processor     *PDFLLMProcessor
	batchSize     int
	maxConcurrent int
}

// NewBatchProcessor åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
func NewBatchProcessor(processor *PDFLLMProcessor) *BatchProcessor {
	return &BatchProcessor{
		processor:     processor,
		batchSize:     100, // æ¯æ‰¹100æ¡æ•°æ®
		maxConcurrent: 8,   // æœ€å¤š8ä¸ªå¹¶å‘
	}
}

// ProcessPDFDataConcurrently å¹¶å‘å¤„ç†PDFæ•°æ®
func (b *BatchProcessor) ProcessPDFDataConcurrently(ctx context.Context, pdfData map[string]interface{}) ([]map[string]interface{}, error) {
	fmt.Printf("DEBUG: ProcessPDFDataConcurrently å¼€å§‹æ‰§è¡Œ\n")

	// 1. æŒ‰ç…§ç¼–ç å‰ç¼€åˆ†ç»„ï¼ˆå¦‚ 1-xx, 2-xx, 3-xxï¼‰
	groups := b.groupByCodePrefix(pdfData)
	fmt.Printf("DEBUG: åˆ†ç»„å®Œæˆï¼Œå…± %d ä¸ªåˆ†ç»„\n", len(groups))

	// 2. åˆ›å»ºç»“æœæ”¶é›†é€šé“
	resultCh := make(chan []map[string]interface{}, len(groups))
	errorCh := make(chan error, len(groups))
	fmt.Printf("DEBUG: é€šé“åˆ›å»ºå®Œæˆ\n")

	// 3. ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
	sem := make(chan struct{}, b.maxConcurrent)
	var wg sync.WaitGroup
	fmt.Printf("DEBUG: å¼€å§‹å¯åŠ¨ %d ä¸ªå¹¶å‘goroutine\n", len(groups))

	// 4. å¹¶å‘å¤„ç†æ¯ä¸ªç»„
	for prefix, groupData := range groups {
		wg.Add(1)
		fmt.Printf("DEBUG: å¯åŠ¨goroutineå¤„ç†åˆ†ç»„ %s\n", prefix)
		go func(prefix string, data map[string]interface{}) {
			defer wg.Done()
			defer fmt.Printf("DEBUG: åˆ†ç»„ %s goroutine ç»“æŸ\n", prefix)

			fmt.Printf("DEBUG: åˆ†ç»„ %s è·å–ä¿¡å·é‡\n", prefix)
			// è·å–ä¿¡å·é‡
			sem <- struct{}{}
			defer func() {
				<-sem
				fmt.Printf("DEBUG: åˆ†ç»„ %s é‡Šæ”¾ä¿¡å·é‡\n", prefix)
			}()

			fmt.Printf("DEBUG: åˆ†ç»„ %s å¼€å§‹è°ƒç”¨processSingleGroup\n", prefix)
			// å¤„ç†è¿™ä¸€ç»„æ•°æ®
			result, err := b.processSingleGroup(ctx, prefix, data)
			if err != nil {
				fmt.Printf("DEBUG: åˆ†ç»„ %s å¤„ç†å¤±è´¥: %v\n", prefix, err)
				errorCh <- fmt.Errorf("å¤„ç†ç»„ %s å¤±è´¥: %w", prefix, err)
				return
			}

			fmt.Printf("DEBUG: åˆ†ç»„ %s å¤„ç†æˆåŠŸï¼Œå‘é€ç»“æœ\n", prefix)
			resultCh <- result
		}(prefix, groupData)
	}

	fmt.Printf("DEBUG: æ‰€æœ‰goroutineå·²å¯åŠ¨ï¼Œå¼€å§‹ç­‰å¾…å®Œæˆ\n")

	// ç­‰å¾…æ‰€æœ‰goroutineå®Œæˆ
	go func() {
		fmt.Printf("DEBUG: å¼€å§‹ç­‰å¾…æ‰€æœ‰goroutineå®Œæˆ\n")
		wg.Wait()
		fmt.Printf("DEBUG: æ‰€æœ‰goroutineå®Œæˆï¼Œå…³é—­é€šé“\n")
		close(resultCh)
		close(errorCh)
	}()

	// 5. æ”¶é›†ç»“æœ
	fmt.Printf("DEBUG: å¼€å§‹æ”¶é›†ç»“æœ\n")
	var allResults []map[string]interface{}
	var errors []error

	for {
		select {
		case result, ok := <-resultCh:
			if !ok {
				fmt.Printf("DEBUG: resultCh å·²å…³é—­\n")
				resultCh = nil
			} else {
				fmt.Printf("DEBUG: æ”¶åˆ°ç»“æœï¼Œé•¿åº¦: %d\n", len(result))
				allResults = append(allResults, result...)
			}
		case err, ok := <-errorCh:
			if !ok {
				fmt.Printf("DEBUG: errorCh å·²å…³é—­\n")
				errorCh = nil
			} else {
				fmt.Printf("DEBUG: æ”¶åˆ°é”™è¯¯: %v\n", err)
				errors = append(errors, err)
			}
		}

		if resultCh == nil && errorCh == nil {
			fmt.Printf("DEBUG: æ‰€æœ‰é€šé“å·²å…³é—­ï¼Œé€€å‡ºæ”¶é›†å¾ªç¯\n")
			break
		}
	}

	fmt.Printf("DEBUG: ç»“æœæ”¶é›†å®Œæˆï¼ŒallResultsé•¿åº¦: %d, errorsé•¿åº¦: %d\n", len(allResults), len(errors))

	// æ£€æŸ¥é”™è¯¯
	if len(errors) > 0 {
		fmt.Printf("DEBUG: å‘ç°é”™è¯¯ï¼Œè¿”å›å¤±è´¥: %v\n", errors)
		return allResults, fmt.Errorf("éƒ¨åˆ†ç»„å¤„ç†å¤±è´¥: %v", errors)
	}

	fmt.Printf("DEBUG: ProcessPDFDataConcurrently æˆåŠŸå®Œæˆ\n")
	return allResults, nil
}

// groupByCodePrefix æŒ‰ç¼–ç å‰ç¼€åˆ†ç»„
func (b *BatchProcessor) groupByCodePrefix(pdfData map[string]interface{}) map[string]map[string]interface{} {
	groups := make(map[string]map[string]interface{})

	// é¦–å…ˆå°è¯•PDFæœåŠ¡æ ¼å¼ {"occupation_codes": [...]}
	var items []interface{}
	if occupationCodes, ok := pdfData["occupation_codes"].([]interface{}); ok {
		items = occupationCodes
		fmt.Printf("DEBUG: groupByCodePrefix æ‰¾åˆ°occupation_codesæ•°ç»„ï¼Œé•¿åº¦: %d\n", len(items))
	} else if itemsArray, ok := pdfData["items"].([]interface{}); ok {
		// å¤‡ç”¨ï¼šå°è¯•itemsæ ¼å¼
		items = itemsArray
		fmt.Printf("DEBUG: groupByCodePrefix æ‰¾åˆ°itemsæ•°ç»„ï¼Œé•¿åº¦: %d\n", len(items))
	} else {
		// å¦‚æœä¸æ˜¯é¢„æœŸæ ¼å¼ï¼Œä½œä¸ºå•ä¸ªç»„å¤„ç†
		fmt.Printf("DEBUG: groupByCodePrefix æœªæ‰¾åˆ°occupation_codesæˆ–itemså­—æ®µï¼Œä½¿ç”¨allç»„\n")
		groups["all"] = pdfData
		return groups
	}

	for _, item := range items {
		itemMap, ok := item.(map[string]interface{})
		if !ok {
			continue
		}

		code, ok := itemMap["code"].(string)
		if !ok {
			continue
		}

		// è·å–ä¸»åˆ†ç±»å‰ç¼€ï¼ˆå¦‚ "1", "2", "3" ç­‰ï¼‰
		prefix := getMainCategory(code)

		if groups[prefix] == nil {
			groups[prefix] = map[string]interface{}{
				"occupation_codes": []interface{}{},
			}
		}

		groupItems := groups[prefix]["occupation_codes"].([]interface{})
		groups[prefix]["occupation_codes"] = append(groupItems, item)
	}

	return groups
}

// getMainCategory è·å–ä¸»åˆ†ç±»
func getMainCategory(code string) string {
	// å¤„ç†ä¸åŒæ ¼å¼çš„ç¼–ç 
	// "1-01-01-01" -> "1"
	// "2-03" -> "2"
	parts := strings.Split(code, "-")
	if len(parts) > 0 {
		return parts[0]
	}

	// å¦‚æœæ²¡æœ‰è¿å­—ç¬¦ï¼Œå–ç¬¬ä¸€ä¸ªå­—ç¬¦
	if len(code) > 0 {
		return string(code[0])
	}

	return "unknown"
}

// processSingleGroup å¤„ç†å•ä¸ªåˆ†ç»„
func (b *BatchProcessor) processSingleGroup(ctx context.Context, prefix string, data map[string]interface{}) ([]map[string]interface{}, error) {
	fmt.Printf("DEBUG: processSingleGroup å¼€å§‹å¤„ç†åˆ†ç»„ %s\n", prefix)

	// ç¬¬ä¸€æ­¥ï¼šæå–æ ¸å¿ƒå­—æ®µ(codeå’Œname)ï¼Œå‡å°‘tokenä½¿ç”¨
	coreData := extractCoreFields(data)
	fmt.Printf("DEBUG: åˆ†ç»„ %s æå–æ ¸å¿ƒå­—æ®µå®Œæˆ\n", prefix)

	// è°ƒè¯•ï¼šè®°å½•æå–çš„æ ¸å¿ƒå­—æ®µæ•°é‡
	if items, ok := coreData["items"].([]interface{}); ok {
		fmt.Printf("DEBUG: åˆ†ç»„ %s æå–äº† %d ä¸ªæ ¸å¿ƒæ¡ç›®ï¼ˆåªåŒ…å«codeå’Œnameï¼‰\n", prefix, len(items))
		if len(items) > 0 {
			// æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ¡ç›®çš„ç»“æ„
			if firstItem, ok := items[0].(map[string]interface{}); ok {
				fields := make([]string, 0, len(firstItem))
				for key := range firstItem {
					fields = append(fields, key)
				}
				fmt.Printf("DEBUG: æ ¸å¿ƒå­—æ®µåŒ…å«: %v\n", fields)
			}
		}
	}

	fmt.Printf("DEBUG: åˆ†ç»„ %s å³å°†æ„å»ºprompt - æ£€æŸ¥ç‚¹A\n", prefix)

	// æ£€æŸ¥contextçŠ¶æ€
	select {
	case <-ctx.Done():
		fmt.Printf("DEBUG: åˆ†ç»„ %s contextå·²å–æ¶ˆ: %v\n", prefix, ctx.Err())
		return nil, ctx.Err()
	default:
		fmt.Printf("DEBUG: åˆ†ç»„ %s contextæ­£å¸¸\n", prefix)
	}

	fmt.Printf("DEBUG: åˆ†ç»„ %s å¼€å§‹æ„å»ºprompt\n", prefix)
	// æ„å»ºé’ˆå¯¹è¿™ä¸ªåˆ†ç»„çš„promptï¼ŒåªåŒ…å«æ ¸å¿ƒå­—æ®µ
	prompt := fmt.Sprintf(`ä½ æ˜¯ä¸€åæ•°æ®æ¸…æ´—ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è±¡åŒ…å«ç¼–ç ï¼ˆcodeï¼‰ã€åç§°ï¼ˆnameï¼‰åŠå…¶ä»–å…ƒæ•°æ®ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä»¥ä¸‹è§„åˆ™ï¼Œä¸ºæ¯ä¸ªå”¯ä¸€çš„ç¼–ç ï¼ˆcodeï¼‰ä»å…¶å…³è”çš„åç§°åˆ—è¡¨ä¸­ï¼Œé€‰å‡ºæœ€å‡†ç¡®ã€æœ€ç²¾ç‚¼çš„èŒä¸šåç§°ã€‚

è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™è¿›è¡Œåˆ¤æ–­ï¼š

1.  **åˆ†ç»„å¤„ç†**ï¼šå°†åˆ—è¡¨ä¸­çš„æ•°æ®æŒ‰ code å­—æ®µè¿›è¡Œåˆ†ç»„ã€‚
2.  **è¯­ä¹‰ç»„åˆåˆ¤æ–­**ï¼š
    * **ä¼˜å…ˆé€‰æ‹©**ï¼šå¦‚æœä¸€ä¸ª code å¯¹åº”çš„å¤šä¸ª name ä¸­ï¼Œåªæœ‰ä¸€ä¸ªæ˜¯å®Œæ•´çš„ã€åè¯æ€§çš„èŒä¸šæˆ–å®ä½“åç§°ï¼Œé‚£ä¹ˆè¿™ä¸€ä¸ªå°±æ˜¯æ­£ç¡®çš„åç§°ã€‚
    * **æ¬¡è¦æ’é™¤**ï¼šå¦‚æœä¸€ä¸ª code ä¸‹çš„åç§°åŒ…å«"æœ¬å°ç±»åŒ…æ‹¬ä¸‹åˆ—èŒä¸š"ã€"è¿›è¡Œ..."æˆ–"æ‹…ä»»..."ç­‰æè¿°æ€§æˆ–åŠ¨è¯æ€§çŸ­è¯­ï¼Œåˆ™è¿™äº›åç§°åº”è¢«æ’é™¤ã€‚å®ƒä»¬æ˜¯è¾…åŠ©æ€§è¯´æ˜ï¼Œä¸æ˜¯æœ€ç»ˆçš„èŒä¸šåç§°ã€‚
    * **å®Œæ•´æ€§ä¼˜å…ˆ**ï¼šå¯¹äºåƒ"èˆªå¤©åŠ¨åŠ›è£…ç½®åˆ¶é€ å·¥"å’Œ"èˆªå¤©åŠ¨åŠ›è£…ç½®åˆ¶é€ å·¥ç¨‹æŠ€æœ¯äººå‘˜"è¿™æ ·çš„æƒ…å†µï¼Œå¦‚æœ"èˆªå¤©åŠ¨åŠ›è£…ç½®åˆ¶é€ å·¥ç¨‹æŠ€æœ¯äººå‘˜"æ˜¯å®Œæ•´çš„ï¼Œè€Œå¦ä¸€ä¸ªæ˜¯æˆªæ–­çš„ï¼ˆæ ¹æ®æ–‡æœ¬å†…å®¹åˆ¤æ–­ï¼‰ï¼Œåˆ™ä¼˜å…ˆé€‰æ‹©å®Œæ•´çš„åç§°ã€‚
3.  **æœ€ç»ˆè¾“å‡º**ï¼šä»¥ code: name çš„JSONæ ¼å¼è¾“å‡ºæœ€ç»ˆç¡®è®¤çš„è¯è¡¨åˆ—è¡¨ã€‚

è¯·ä½¿ç”¨æ­¤æ–¹æ³•å¤„ç†ä»¥ä¸‹JSONæ•°æ®ï¼Œå¹¶ä»…è¿”å›æœ€ç»ˆç»“æœã€‚

%s

è¾“å‡ºJSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š
[
  {
    "code": "èŒä¸šç¼–ç ",
    "name": "èŒä¸šåç§°",
    "confidence": "ç½®ä¿¡åº¦(0-1)"
  }
]
`, jsonString(coreData))

	fmt.Printf("DEBUG: åˆ†ç»„ %s å¼€å§‹è°ƒç”¨LLMæœåŠ¡\n", prefix)
	// è°ƒç”¨LLMæœåŠ¡
	result, err := b.processor.callLLMService(ctx, "data_cleaning", prompt)
	if err != nil {
		fmt.Printf("DEBUG: åˆ†ç»„ %s LLMè°ƒç”¨å¤±è´¥: %v\n", prefix, err)
		return nil, err
	}
	fmt.Printf("DEBUG: åˆ†ç»„ %s LLMè°ƒç”¨æˆåŠŸï¼Œç»“æœé•¿åº¦: %d\n", prefix, len(result))
	
	// æ‰“å°LLMåŸå§‹å“åº”ä»¥ä¾¿è°ƒè¯•
	if len(result) > 0 {
		fmt.Printf("ğŸ” [åˆ†ç»„%s-LLMåŸå§‹å“åº”] é•¿åº¦=%d\n", prefix, len(result))
		if len(result) <= 500 {
			fmt.Printf("ğŸ“ [åˆ†ç»„%s-å®Œæ•´å“åº”]:\n%s\n", prefix, result)
		} else {
			// æ‰“å°å¼€å¤´å’Œç»“å°¾
			fmt.Printf("ğŸ“ [åˆ†ç»„%s-å“åº”å¼€å¤´200å­—ç¬¦]:\n%s\n", prefix, result[:200])
			fmt.Printf("ğŸ“ [åˆ†ç»„%s-å“åº”ç»“å°¾200å­—ç¬¦]:\n%s\n", prefix, result[len(result)-200:])
		}
		
		// æ£€æŸ¥æ˜¯å¦å¯èƒ½è¢«æˆªæ–­
		if !strings.HasSuffix(strings.TrimSpace(result), "]") && !strings.HasSuffix(strings.TrimSpace(result), "}") {
			fmt.Printf("âš ï¸ [åˆ†ç»„%s-å¯èƒ½æˆªæ–­] ç»“æœä¸ä»¥}]ç»“å°¾\n", prefix)
		}
	}

	fmt.Printf("DEBUG: åˆ†ç»„ %s å¼€å§‹è§£æç»“æœ\n", prefix)
	// è§£æç»“æœ - å¤„ç†ä¸¤ç§æ ¼å¼ï¼š{"items": [...]} æˆ– ç›´æ¥çš„JSONæ•°ç»„
	var cleanedData []map[string]interface{}
	
	// ä¸è¦æ¸…ç†ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç»“æœï¼ˆå› ä¸ºextractJSONå¯èƒ½ç ´åwrapperæ ¼å¼ï¼‰
	cleanResult := strings.TrimSpace(result)
	
	// ç§»é™¤å¯èƒ½çš„markdownæ ‡è®°
	if strings.HasPrefix(cleanResult, "```json") {
		cleanResult = strings.TrimPrefix(cleanResult, "```json")
		cleanResult = strings.TrimSuffix(cleanResult, "```")
		cleanResult = strings.TrimSpace(cleanResult)
	} else if strings.HasPrefix(cleanResult, "```") {
		cleanResult = strings.TrimPrefix(cleanResult, "```")
		cleanResult = strings.TrimSuffix(cleanResult, "```")
		cleanResult = strings.TrimSpace(cleanResult)
	}
	
	if cleanResult != result {
		fmt.Printf("ğŸ”„ [åˆ†ç»„%s-æ¸…ç†Markdown] åŸå§‹é•¿åº¦=%d, æ¸…ç†å=%d\n", prefix, len(result), len(cleanResult))
	}
	
	// æ£€æµ‹JSONæ ¼å¼ - æ ¹æ®ç¬¬ä¸€ä¸ªå­—ç¬¦åˆ¤æ–­
	isArray := strings.HasPrefix(cleanResult, "[")
	isObject := strings.HasPrefix(cleanResult, "{")
	
	fmt.Printf("ğŸ” [åˆ†ç»„%s-æ ¼å¼æ£€æµ‹] isObject=%v, isArray=%v\n", prefix, isObject, isArray)
	
	if isObject {
		// å¯¹è±¡æ ¼å¼ï¼Œå°è¯• wrapper æ ¼å¼
		var resultWrapper struct {
			Items []map[string]interface{} `json:"items"`
		}
		if err := json.Unmarshal([]byte(cleanResult), &resultWrapper); err != nil {
			fmt.Printf("âš ï¸ [åˆ†ç»„%s-wrapperæ ¼å¼å¤±è´¥] é”™è¯¯: %v\n", prefix, err)
			// wrapperæ ¼å¼å¤±è´¥ï¼Œå¯èƒ½æ˜¯å…¶ä»–å¯¹è±¡æ ¼å¼
			return nil, fmt.Errorf("wrapperæ ¼å¼è§£æå¤±è´¥: %v", err)
		}
		cleanedData = resultWrapper.Items
		fmt.Printf("âœ… [åˆ†ç»„%s-è§£ææˆåŠŸ] wrapperæ ¼å¼ï¼Œè·å¾— %d æ¡æ•°æ®\n", prefix, len(cleanedData))
		
	} else if isArray {
		// æ•°ç»„æ ¼å¼
		if err := json.Unmarshal([]byte(cleanResult), &cleanedData); err != nil {
			fmt.Printf("âš ï¸ [åˆ†ç»„%s-æ•°ç»„æ ¼å¼å¤±è´¥] é”™è¯¯: %v\n", prefix, err)
			
			// å°è¯•éƒ¨åˆ†è§£æ
			fmt.Printf("ğŸ”„ [åˆ†ç»„%s-å°è¯•éƒ¨åˆ†è§£æ]...\n", prefix)
			if partialData := tryParsePartialJSON(cleanResult); partialData != nil && len(partialData) > 0 {
				fmt.Printf("âœ… [åˆ†ç»„%s-éƒ¨åˆ†è§£ææˆåŠŸ] è§£æå‡º %d æ¡æ•°æ®\n", prefix, len(partialData))
				return partialData, nil
			}
			return nil, fmt.Errorf("æ•°ç»„æ ¼å¼è§£æå¤±è´¥: %v", err)
		}
		fmt.Printf("âœ… [åˆ†ç»„%s-è§£ææˆåŠŸ] ç›´æ¥æ•°ç»„æ ¼å¼ï¼Œè·å¾— %d æ¡æ•°æ®\n", prefix, len(cleanedData))
		
	} else {
		// å¯èƒ½æ˜¯åŒé‡ç¼–ç çš„å­—ç¬¦ä¸²
		var jsonString string
		if err := json.Unmarshal([]byte(cleanResult), &jsonString); err != nil {
			fmt.Printf("âŒ [åˆ†ç»„%s-æ— æ³•è¯†åˆ«æ ¼å¼] æ—¢ä¸æ˜¯å¯¹è±¡ä¹Ÿä¸æ˜¯æ•°ç»„\n", prefix)
			return nil, fmt.Errorf("æ— æ³•è¯†åˆ«çš„JSONæ ¼å¼")
		}
		
		fmt.Printf("ğŸ”„ [åˆ†ç»„%s-åŒé‡ç¼–ç ] æ£€æµ‹åˆ°JSONå­—ç¬¦ä¸²ï¼ŒäºŒæ¬¡è§£æ...\n", prefix)
		// é€’å½’è°ƒç”¨è‡ªå·±æ¥è§£æ
		return b.processSingleGroup(ctx, prefix, map[string]interface{}{"result": jsonString})
	}
	fmt.Printf("DEBUG: åˆ†ç»„ %s è§£ææˆåŠŸï¼Œæ¸…æ´—åæ•°æ®æ¡æ•°: %d\n", prefix, len(cleanedData))

	return cleanedData, nil
}

// ProcessInBatches åˆ†æ‰¹å¤„ç†æ•°æ®
func (b *BatchProcessor) ProcessInBatches(ctx context.Context, categories []*model.Category) ([]map[string]interface{}, error) {
	// å°†categoriesåˆ†æ‰¹
	batches := b.splitIntoBatches(categories)

	// åˆ›å»ºworkeræ± 
	workerCount := b.maxConcurrent
	if len(batches) < workerCount {
		workerCount = len(batches)
	}

	// åˆ›å»ºä»»åŠ¡é€šé“å’Œç»“æœé€šé“
	taskCh := make(chan []*model.Category, len(batches))
	resultCh := make(chan batchResult, len(batches))

	// å¯åŠ¨workers
	var wg sync.WaitGroup
	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go b.batchWorker(ctx, i, taskCh, resultCh, &wg)
	}

	// åˆ†å‘ä»»åŠ¡
	for _, batch := range batches {
		taskCh <- batch
	}
	close(taskCh)

	// ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
	go func() {
		wg.Wait()
		close(resultCh)
	}()

	// æ”¶é›†ç»“æœ
	var allResults []map[string]interface{}
	errors := []error{}

	for result := range resultCh {
		if result.err != nil {
			errors = append(errors, result.err)
		} else {
			allResults = append(allResults, result.data...)
		}
	}

	if len(errors) > 0 {
		return allResults, fmt.Errorf("æ‰¹å¤„ç†ä¸­æœ‰é”™è¯¯: %v", errors)
	}

	return allResults, nil
}

// splitIntoBatches å°†æ•°æ®åˆ†æ‰¹
func (b *BatchProcessor) splitIntoBatches(categories []*model.Category) [][]*model.Category {
	var batches [][]*model.Category

	// å…ˆæŒ‰å¤§ç±»åˆ†ç»„
	groupedByMain := make(map[string][]*model.Category)

	for _, cat := range categories {
		if cat == nil {
			continue
		}
		mainCat := getMainCategory(cat.Code)
		groupedByMain[mainCat] = append(groupedByMain[mainCat], cat)
	}

	// æ¯ä¸ªå¤§ç±»å†æŒ‰batchSizeåˆ†æ‰¹
	for _, group := range groupedByMain {
		for i := 0; i < len(group); i += b.batchSize {
			end := i + b.batchSize
			if end > len(group) {
				end = len(group)
			}
			batches = append(batches, group[i:end])
		}
	}

	return batches
}

// batchResult æ‰¹å¤„ç†ç»“æœ
type batchResult struct {
	data []map[string]interface{}
	err  error
}

// batchWorker æ‰¹å¤„ç†å·¥ä½œåç¨‹
func (b *BatchProcessor) batchWorker(ctx context.Context, id int, taskCh <-chan []*model.Category, resultCh chan<- batchResult, wg *sync.WaitGroup) {
	defer wg.Done()

	for batch := range taskCh {
		// å¤„ç†ä¸€æ‰¹æ•°æ®
		result, err := b.processBatch(ctx, id, batch)

		select {
		case resultCh <- batchResult{data: result, err: err}:
		case <-ctx.Done():
			return
		}
	}
}

// processBatch å¤„ç†ä¸€æ‰¹æ•°æ®
func (b *BatchProcessor) processBatch(ctx context.Context, workerID int, batch []*model.Category) ([]map[string]interface{}, error) {
	// è½¬æ¢ä¸ºmapæ ¼å¼
	var items []map[string]interface{}
	for _, cat := range batch {
		items = append(items, map[string]interface{}{
			"code":  cat.Code,
			"name":  cat.Name,
			"level": cat.Level,
		})
	}

	// æ„å»ºprompt
	prompt := fmt.Sprintf(`åˆ†æå¹¶ä¼˜åŒ–ä»¥ä¸‹èŒä¸šåˆ†ç±»æ•°æ®ï¼ˆæ‰¹æ¬¡%dï¼Œå…±%dæ¡ï¼‰ï¼š

%s

è¦æ±‚ï¼š
1. éªŒè¯ç¼–ç æ ¼å¼æ­£ç¡®æ€§
2. ä¼˜åŒ–èŒä¸šåç§°è¡¨è¿°
3. ç¡®ä¿å±‚çº§å…³ç³»åˆç†

è¾“å‡ºJSONæ•°ç»„ã€‚`, workerID, len(items), jsonString(items))

	// è°ƒç”¨LLMï¼ˆå¸¦é‡è¯•ï¼‰
	result, err := b.processor.callLLMServiceWithRetry(ctx, "batch_processing", prompt, 3)
	if err != nil {
		return nil, fmt.Errorf("worker %d å¤„ç†å¤±è´¥: %w", workerID, err)
	}

	// è§£æç»“æœ
	var processedData []map[string]interface{}
	if err := json.Unmarshal([]byte(result), &processedData); err != nil {
		return nil, fmt.Errorf("worker %d è§£æç»“æœå¤±è´¥: %w", workerID, err)
	}

	return processedData, nil
}

// OptimizeWithPipeline ä½¿ç”¨pipelineæ¨¡å¼ä¼˜åŒ–å¤„ç†
func (b *BatchProcessor) OptimizeWithPipeline(ctx context.Context, taskID string, categories []*model.Category) error {
	// åˆ›å»ºpipelineé˜¶æ®µ
	stages := []pipelineStage{
		{name: "åˆ†ç»„", fn: b.groupingStage},
		{name: "æ¸…æ´—", fn: b.cleaningStage},
		{name: "éªŒè¯", fn: b.validationStage},
		{name: "åˆå¹¶", fn: b.mergeStage},
	}

	// æ‰§è¡Œpipeline
	data := &pipelineData{
		taskID:     taskID,
		categories: categories,
		results:    make(map[string]interface{}),
	}

	for _, stage := range stages {
		start := time.Now()
		if err := stage.fn(ctx, data); err != nil {
			return fmt.Errorf("pipelineé˜¶æ®µ %s å¤±è´¥: %w", stage.name, err)
		}
		fmt.Printf("Pipelineé˜¶æ®µ %s å®Œæˆï¼Œè€—æ—¶: %v\n", stage.name, time.Since(start))
	}

	return nil
}

// pipelineStage pipelineé˜¶æ®µ
type pipelineStage struct {
	name string
	fn   func(context.Context, *pipelineData) error
}

// pipelineData pipelineæ•°æ®
type pipelineData struct {
	taskID     string
	categories []*model.Category
	results    map[string]interface{}
	mu         sync.RWMutex
}

// groupingStage åˆ†ç»„é˜¶æ®µ
func (b *BatchProcessor) groupingStage(ctx context.Context, data *pipelineData) error {
	// æŒ‰å¤§ç±»åˆ†ç»„
	grouped := make(map[string][]*model.Category)
	for _, cat := range data.categories {
		mainCat := getMainCategory(cat.Code)
		grouped[mainCat] = append(grouped[mainCat], cat)
	}

	data.mu.Lock()
	data.results["grouped"] = grouped
	data.mu.Unlock()

	return nil
}

// cleaningStage æ¸…æ´—é˜¶æ®µ
func (b *BatchProcessor) cleaningStage(ctx context.Context, data *pipelineData) error {
	data.mu.RLock()
	grouped := data.results["grouped"].(map[string][]*model.Category)
	data.mu.RUnlock()

	// å¹¶å‘æ¸…æ´—æ¯ä¸ªç»„
	var wg sync.WaitGroup
	cleanedResults := make(map[string][]map[string]interface{})
	var mu sync.Mutex

	for mainCat, cats := range grouped {
		wg.Add(1)
		go func(mc string, categories []*model.Category) {
			defer wg.Done()

			// æ¸…æ´—è¿™ä¸€ç»„
			cleaned, err := b.ProcessInBatches(ctx, categories)
			if err != nil {
				fmt.Printf("æ¸…æ´—ç»„ %s å¤±è´¥: %v\n", mc, err)
				return
			}

			mu.Lock()
			cleanedResults[mc] = cleaned
			mu.Unlock()
		}(mainCat, cats)
	}

	wg.Wait()

	data.mu.Lock()
	data.results["cleaned"] = cleanedResults
	data.mu.Unlock()

	return nil
}

// validationStage éªŒè¯é˜¶æ®µ
func (b *BatchProcessor) validationStage(ctx context.Context, data *pipelineData) error {
	// éªŒè¯æ¸…æ´—åçš„æ•°æ®
	data.mu.RLock()
	cleaned := data.results["cleaned"].(map[string][]map[string]interface{})
	data.mu.RUnlock()

	validated := make(map[string][]map[string]interface{})

	for mainCat, items := range cleaned {
		validItems := []map[string]interface{}{}
		for _, item := range items {
			if b.validateItem(item) {
				validItems = append(validItems, item)
			}
		}
		validated[mainCat] = validItems
	}

	data.mu.Lock()
	data.results["validated"] = validated
	data.mu.Unlock()

	return nil
}

// validateItem éªŒè¯å•ä¸ªé¡¹ç›®
func (b *BatchProcessor) validateItem(item map[string]interface{}) bool {
	// éªŒè¯å¿…è¦å­—æ®µ
	_, hasCode := item["code"].(string)
	_, hasName := item["name"].(string)

	return hasCode && hasName
}

// mergeStage åˆå¹¶é˜¶æ®µ
func (b *BatchProcessor) mergeStage(ctx context.Context, data *pipelineData) error {
	data.mu.RLock()
	validated := data.results["validated"].(map[string][]map[string]interface{})
	data.mu.RUnlock()

	// åˆå¹¶æ‰€æœ‰ç»“æœ
	var allResults []map[string]interface{}

	// æŒ‰ç¼–ç æ’åºçš„é”®
	var keys []string
	for k := range validated {
		keys = append(keys, k)
	}
	sort.Strings(keys)

	// æŒ‰é¡ºåºåˆå¹¶
	for _, key := range keys {
		allResults = append(allResults, validated[key]...)
	}

	data.mu.Lock()
	data.results["final"] = allResults
	data.mu.Unlock()

	return nil
}


// extractCoreFields æå–æ ¸å¿ƒå­—æ®µ(codeå’Œname)ï¼Œå‡å°‘tokenä½¿ç”¨é‡
func extractCoreFields(data map[string]interface{}) map[string]interface{} {
	// è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°è¾“å…¥æ•°æ®ç»“æ„çš„é”®
	var keys []string
	for k := range data {
		keys = append(keys, k)
	}
	fmt.Printf("DEBUG: extractCoreFields è¾“å…¥æ•°æ®é”®: %v\n", keys)

	coreData := map[string]interface{}{
		"items": []interface{}{},
	}

	// é¦–å…ˆå°è¯•è·å–PDFæœåŠ¡æ ¼å¼çš„æ•°æ® {"occupation_codes": [...]}
	var items []interface{}
	if occupationCodes, ok := data["occupation_codes"].([]interface{}); ok {
		items = occupationCodes
		fmt.Printf("DEBUG: æ‰¾åˆ°occupation_codesæ•°ç»„ï¼Œé•¿åº¦: %d\n", len(items))
	} else if itemsArray, ok := data["items"].([]interface{}); ok {
		// å¤‡ç”¨ï¼šå°è¯•itemsæ ¼å¼
		items = itemsArray
		fmt.Printf("DEBUG: æ‰¾åˆ°itemsæ•°ç»„ï¼Œé•¿åº¦: %d\n", len(items))
	} else {
		fmt.Printf("DEBUG: æ— æ³•æ‰¾åˆ°occupation_codesæˆ–itemså­—æ®µ\n")
		return coreData
	}

	var coreItems []interface{}
	// é™åˆ¶æœ€å¤šå¤„ç†å‰5ä¸ªæ¡ç›®è¿›è¡ŒéªŒè¯æµ‹è¯•
	maxItems := 500
	processedCount := 0

	for _, item := range items {
		if processedCount >= maxItems {
			break
		}

		itemMap, ok := item.(map[string]interface{})
		if !ok {
			continue
		}

		// åªæå–codeå’Œnameå­—æ®µ
		coreItem := map[string]interface{}{}

		if code, exists := itemMap["code"]; exists {
			coreItem["code"] = code
		}

		if name, exists := itemMap["name"]; exists {
			coreItem["name"] = name
		}

		// åªæœ‰å½“codeæˆ–nameå­˜åœ¨æ—¶æ‰æ·»åŠ 
		if len(coreItem) > 0 {
			coreItems = append(coreItems, coreItem)
			processedCount++
		}
	}

	fmt.Printf("DEBUG: é™åˆ¶å¤„ç†æ¡ç›®æ•°é‡ï¼ŒåŸå§‹: %d, å¤„ç†: %d, æå–: %d\n", len(items), processedCount, len(coreItems))

	coreData["items"] = coreItems
	return coreData
}
